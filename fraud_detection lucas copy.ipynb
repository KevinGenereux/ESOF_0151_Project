{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "Python 3.6.5 64-bit (conda)",
      "display_name": "Python 3.6.5 64-bit (conda)",
      "metadata": {
        "interpreter": {
          "hash": "e73f067f1c59be12cfda938665c6a960a5d3ec55ce3a05feea7c329cb1f7e9af"
        }
      }
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5-final"
    },
    "colab": {
      "name": "david-fraud-detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "piJFKIoM5jfX"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import collections\n",
        "from time import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn import preprocessing\n",
        "from sklearn import metrics\n",
        "\n",
        "# Models\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "\n",
        "#for the NN\n",
        "from numpy import loadtxt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras import backend as K\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFoUe-Ed5jfg"
      },
      "source": [
        "def prepare_inputs_and_outputs(data):\n",
        "    \n",
        "    # Prepare & save the inputs and outputs features\n",
        "    features = data.drop(['isFraud','TransactionID'], axis = 1)\n",
        "    labels = data[['isFraud']]\n",
        "    \n",
        "    return features, labels"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGBJ2vBk5jfj"
      },
      "source": [
        "def get_missing_data_percentage(data):\n",
        "    \n",
        "    # where mvp = missing value percentages\n",
        "    mvp = data.isnull().sum() * 100 / len(data)\n",
        "    mvp = pd.DataFrame({'Feature': data.columns,'Percentage': mvp})\n",
        "    \n",
        "    return mvp.sort_values(by ='Percentage', ascending=False)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCxN6o7n5jfn"
      },
      "source": [
        "def drop_high_missing_data_columns(mvd, data, threshold):\n",
        "    # Where \"mvd\" = missing value data\n",
        "    # Get names of indexes for which column missing data is over 50%\n",
        "    high_missing_data_cols = mvd[mvd['Percentage'] > threshold].index\n",
        "\n",
        "    for col_name in range(len(high_missing_data_cols)):\n",
        "        del data[high_missing_data_cols[col_name]] # Delete rows from dataFrame??? or columns\n",
        "    \n",
        "    return data"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQoerlZK5jfr"
      },
      "source": [
        "def drop_one_value_columns(data):\n",
        "    \n",
        "    # Drop columns with only 1 unique value.\n",
        "    for column in data.columns:\n",
        "        if len(data[column].unique()) == 1:\n",
        "            #print(traindata[column].name)\n",
        "            data.drop(column,inplace=True,axis=1)\n",
        "            \n",
        "    return data"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPIc725i5jfx"
      },
      "source": [
        "def getCategoricalFeatures(data):\n",
        "    columns = list(data)\n",
        "    result = []\n",
        "    for c in columns:\n",
        "        if data.dtypes[c] == np.object:\n",
        "            result.append(c)\n",
        "    return data[result]\n",
        "\n",
        "def getNumericalFeatures(data):\n",
        "    columns = list(data)\n",
        "    result = []\n",
        "    for c in columns: \n",
        "        if data.dtypes[c] != np.object:\n",
        "            result.append(c) \n",
        "    return data[result]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3hxDcdW5jf0"
      },
      "source": [
        "def drop_high_correlation_features(data, threshold):\n",
        "\n",
        "    corr_matrix = data.corr()\n",
        "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "    to_drop = [column for column in upper.columns if any(abs(upper[column]) > threshold)]\n",
        "    data = data.drop(columns = to_drop)\n",
        "    \n",
        "    return data"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSE1yVrx5jf4"
      },
      "source": [
        "def label_encode_categorical_features(data):\n",
        "    encoder_dict = collections.defaultdict(LabelEncoder)\n",
        "    data = data.apply(lambda x: encoder_dict[x.name].fit_transform(x))\n",
        "    return data"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE1lS-qa5jf8"
      },
      "source": [
        "def split_data(features, labels):\n",
        "    \n",
        "    # Data Splitting: 60% for training, 20% for validation and 20% for testing.\n",
        "    X_train, X_test, Y_train, y_test = train_test_split(features, labels, test_size=0.4)\n",
        "    X_validation, X_test, Y_validation, y_test = train_test_split(X_test, y_test, test_size=0.5)\n",
        "    \n",
        "    return X_train, Y_train, X_test, y_test, X_validation, Y_validation"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDfUoKCN5jf_"
      },
      "source": [
        "def selectkbestfeatures(X_train, Y_train, X_validation, X_test, numberOfFeatures):\n",
        "\n",
        "    fit = SelectKBest(score_func=f_classif, k=numberOfFeatures).fit(X_train, Y_train)\n",
        "\n",
        "    X_train = fit.transform(X_train)\n",
        "    X_validation = fit.transform(X_validation)\n",
        "    X_test = fit.transform(X_test)\n",
        "\n",
        "    # Get column names from the best features\n",
        "    X_train_cols = fit.get_support(indices=True)\n",
        "    X_validation_cols = fit.get_support(indices=True)\n",
        "    X_test_cols = fit.get_support(indices=True)\n",
        "\n",
        "    X_train = pd.DataFrame(X_train, columns=X_train_cols)\n",
        "    X_validation = pd.DataFrame(X_validation, columns=X_validation_cols)\n",
        "    X_test = pd.DataFrame(X_test, columns=X_test_cols)\n",
        "\n",
        "    # Create new dataframes with the column names\n",
        "    #X_train = X_train.iloc[:,X_train_cols]\n",
        "    #X_validation = X_validation.iloc[:,X_validation_cols]\n",
        "    #X_test = X_test.iloc[:,X_test_cols]\n",
        "\n",
        "    return X_train, X_validation, X_test"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' \\nclass NN(object):\\n    def __init__(self):\\n        self.inputLayerSize=24\\n        self.hiddenLayerSize=50\\n        self.outputLayerSize=1\\n\\n        self.weight1=np.random.randn(self.inputLayerSize, self.hiddenLayerSize)\\n        self.weight2=np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\\n\\n    def forwardprop(self,X):\\n        self.z2=np.dot(Xm, self.weight1)\\n        self.a2=self.sigmoid(self.z2)\\n        self.z3=np,dot(self.a2, self.weight2)\\n        yhat=self.sigmoid(self.z3)\\n        return yhat\\n    \\n    def sigmoid(self,y):\\n        ans=1/(1+np.exp(-y))\\n        return ans\\n\\n    def sigmoidPrime(self,z):\\n        return np,exp(-z)/((1+np.exp(z))**2)\\n\\n    def costFunction(self, X, y):\\n        self.yhat=self.forwardprop(X)\\n        j=0.5*sum((y-self.yhat)**2)\\n        return j\\n\\n    def costPrime(self,X,y):\\n        self.yaht=self.forwardprop(X)\\n        delta3=np.multiply(-(y-self.yhat), self.sigmoidPrime(self.z3))\\n        dHdW2=np.dot(self.a2.T, delta3)\\n\\n        delta2=np.dot(delta3, self.weight2)*self.sigmoidPrime(self.z2)\\n        dJdW1=np.dot(X.T, delta2)\\n        return dJdW1, dJdW2\\n     '"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# functions and such to get the Neural Network up and running \n",
        "#this will be an example of a simple perceptron NN using the sigmoid function \n",
        "\n",
        "\n",
        "\"\"\" \n",
        "class NN(object):\n",
        "    def __init__(self):\n",
        "        self.inputLayerSize=24\n",
        "        self.hiddenLayerSize=50\n",
        "        self.outputLayerSize=1\n",
        "\n",
        "        self.weight1=np.random.randn(self.inputLayerSize, self.hiddenLayerSize)\n",
        "        self.weight2=np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
        "\n",
        "    def forwardprop(self,X):\n",
        "        self.z2=np.dot(Xm, self.weight1)\n",
        "        self.a2=self.sigmoid(self.z2)\n",
        "        self.z3=np,dot(self.a2, self.weight2)\n",
        "        yhat=self.sigmoid(self.z3)\n",
        "        return yhat\n",
        "    \n",
        "    def sigmoid(self,y):\n",
        "        ans=1/(1+np.exp(-y))\n",
        "        return ans\n",
        "\n",
        "    def sigmoidPrime(self,z):\n",
        "        return np,exp(-z)/((1+np.exp(z))**2)\n",
        "\n",
        "    def costFunction(self, X, y):\n",
        "        self.yhat=self.forwardprop(X)\n",
        "        j=0.5*sum((y-self.yhat)**2)\n",
        "        return j\n",
        "\n",
        "    def costPrime(self,X,y):\n",
        "        self.yaht=self.forwardprop(X)\n",
        "        delta3=np.multiply(-(y-self.yhat), self.sigmoidPrime(self.z3))\n",
        "        dHdW2=np.dot(self.a2.T, delta3)\n",
        "\n",
        "        delta2=np.dot(delta3, self.weight2)*self.sigmoidPrime(self.z2)\n",
        "        dJdW1=np.dot(X.T, delta2)\n",
        "        return dJdW1, dJdW2\n",
        "     \"\"\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' #Tensorflow \\nfrom keras.models import Sequential\\nimport tensorflow.compat.v1 as tf\\ntf.disable_v2_behavior()\\nlearning_rate=0.5\\nepochs=10 \\nbatch_size=100\\nlayer_nodes=300\\n#set the input and output layer with 24 types or columns coming in and two possible outcomes\\nx=tf.placeholder(tf.float32,[None, 24])\\ny=tf.placeholder(tf.float32,[None, 2])\\n#set weights and biases \\n#input to hidden\\nw1=tf.Variable(tf.random_normal([24,layer_nodes],stddev=0.03), name=\\'w1\\')\\nb1=tf.Variable(tf.random_normal([layer_nodes]), name=\\'b1\\')\\n#hidden to output\\nw2=tf.Variable(tf.random_normal([layer_nodes,2],stddev=0.03), name=\\'w2\\')\\nb2=tf.Variable(tf.random_normal([2]), name=\\'b2\\')\\n\\nhidden_out=tf.add(tf.matmul(x,w1),b1)\\nhidden_out=tf.nn.relu(hidden_out)\\n\\ny_=tf.nn.softmax(tf.add(tf.matmul(hidden_out,w2),b2))\\n\\ny_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)\\ncross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)+ (1 - y) * tf.log(1 - y_clipped), axis=1))\\n\\noptimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\\n\\ninit_op=tf.global_variables_initializer()\\n\\ncorrect_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\\nNNInput=np.array(X_train)\\n# start the session\\nwith tf.Session() as sess:\\n   # initialise the variables\\n   sess.run(init_op)\\n   total_batch = int(len(NNInput) / batch_size)\\n   for epoch in range(epochs):\\n        avg_cost = 0\\n        for i in range(total_batch):\\n            batch_x, batch_y = NNInput.next_batch(batch_size=batch_size)\\n            _, c = sess.run([optimiser, cross_entropy], \\n                         feed_dict={x: batch_x, y: batch_y})\\n            avg_cost += c / total_batch\\n        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost))\\n   print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})) '"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "\"\"\" #Tensorflow \n",
        "from keras.models import Sequential\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "learning_rate=0.5\n",
        "epochs=10 \n",
        "batch_size=100\n",
        "layer_nodes=300\n",
        "#set the input and output layer with 24 types or columns coming in and two possible outcomes\n",
        "x=tf.placeholder(tf.float32,[None, 24])\n",
        "y=tf.placeholder(tf.float32,[None, 2])\n",
        "#set weights and biases \n",
        "#input to hidden\n",
        "w1=tf.Variable(tf.random_normal([24,layer_nodes],stddev=0.03), name='w1')\n",
        "b1=tf.Variable(tf.random_normal([layer_nodes]), name='b1')\n",
        "#hidden to output\n",
        "w2=tf.Variable(tf.random_normal([layer_nodes,2],stddev=0.03), name='w2')\n",
        "b2=tf.Variable(tf.random_normal([2]), name='b2')\n",
        "\n",
        "hidden_out=tf.add(tf.matmul(x,w1),b1)\n",
        "hidden_out=tf.nn.relu(hidden_out)\n",
        "\n",
        "y_=tf.nn.softmax(tf.add(tf.matmul(hidden_out,w2),b2))\n",
        "\n",
        "y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)\n",
        "cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)+ (1 - y) * tf.log(1 - y_clipped), axis=1))\n",
        "\n",
        "optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
        "\n",
        "init_op=tf.global_variables_initializer()\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "NNInput=np.array(X_train)\n",
        "# start the session\n",
        "with tf.Session() as sess:\n",
        "   # initialise the variables\n",
        "   sess.run(init_op)\n",
        "   total_batch = int(len(NNInput) / batch_size)\n",
        "   for epoch in range(epochs):\n",
        "        avg_cost = 0\n",
        "        for i in range(total_batch):\n",
        "            batch_x, batch_y = NNInput.next_batch(batch_size=batch_size)\n",
        "            _, c = sess.run([optimiser, cross_entropy], \n",
        "                         feed_dict={x: batch_x, y: batch_y})\n",
        "            avg_cost += c / total_batch\n",
        "        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost))\n",
        "   print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})) \"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' predictions=model.predict_classes(X_test)\\ncheck=np.array(Y_test)\\ncount=0\\nfor i in range(predictions.size):\\n   if(predictions[i]==check[i]):\\n       count=count+1\\n\\nprint(count)\\nacc=count/predictions.size\\nprint(acc) '"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "\"\"\" predictions=model.predict_classes(X_test)\n",
        "check=np.array(Y_test)\n",
        "count=0\n",
        "for i in range(predictions.size):\n",
        "   if(predictions[i]==check[i]):\n",
        "       count=count+1\n",
        "\n",
        "print(count)\n",
        "acc=count/predictions.size\n",
        "print(acc) \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' predictions=model.predict_classes(NNIN)\\ncheck=np.array(NNans)\\ncount=0\\ncount1=0\\nfor i in range(predictions.size):\\n    if(predictions[i]==check[i]):\\n       count=count+1\\n    if(check[i]==1):\\n       print(predictions[i])\\n\\n\\nprint(count)\\nacc=count/predictions.size\\nprint(acc)\\n '"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "\"\"\" predictions=model.predict_classes(NNIN)\n",
        "check=np.array(NNans)\n",
        "count=0\n",
        "count1=0\n",
        "for i in range(predictions.size):\n",
        "    if(predictions[i]==check[i]):\n",
        "       count=count+1\n",
        "    if(check[i]==1):\n",
        "       print(predictions[i])\n",
        "\n",
        "\n",
        "print(count)\n",
        "acc=count/predictions.size\n",
        "print(acc)\n",
        " \"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DXZ6J2a5jgB"
      },
      "source": [
        "def evaluate_model(name, model, features, labels):\n",
        "    \n",
        "    start = time()\n",
        "    pred = model.predict(features)\n",
        "    end = time()\n",
        "    \n",
        "    # Print the confusion matrix\n",
        "    print(metrics.confusion_matrix(labels, pred))\n",
        "\n",
        "    # Print the precision and recall, among other metrics\n",
        "    print(metrics.classification_report(labels, pred, digits=3))\n",
        "    \n",
        "    print(name+\" Accuracy - \"+str(round(accuracy_score(labels, pred), 3) * 100)+\"%\")\n",
        "    print(name+\" Precision - \"+str(round(precision_score(labels, pred, average='micro'), 3) * 100)+\"%\")\n",
        "    print(name+\" Recall - \"+str(round(recall_score(labels, pred, average='micro'), 3) * 100)+\"%\")\n",
        "    print(name+\" F1 Score - \"+str(round(f1_score(labels, pred, average='micro'), 3) * 100)+\"%\")\n",
        "    print(name+\" Latency - \"+str(round((end - start) * 1000, 1))+\"ms \\n\")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "source": [
        "This will be the start of the data analytics and will make use of the above functions "
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# read and combine train data by the TransactionID\n",
        "#this has been changed to read from the local machine instad of from google drive as is done with colab \n",
        "train_identity = pd.read_csv(\"D:\\School\\Fifth Year\\Large Scale Data Analytics\\Project\\ieee-fraud-detection\\\\train_identity.csv\")\n",
        "train_transaction = pd.read_csv(\"D:\\School\\Fifth Year\\Large Scale Data Analytics\\Project\\ieee-fraud-detection\\\\train_transaction.csv\")\n",
        "#combine the data so we can go through the whole thing \n",
        "traindata = pd.merge(train_transaction,train_identity, on='TransactionID', how='left',left_index=True,right_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxwvqD1k7R1Q"
      },
      "source": [
        "# Separate Features & Labels\n",
        "train_features, train_labels = prepare_inputs_and_outputs(traindata)#used to be at the top but I moved it in order to have the"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANvotlOt7di1"
      },
      "source": [
        "# [PREPROCESSING STAGE 1] - DATA CLEANING\n",
        "\n",
        "# Examine the percentage of missing data for all feature in the training data\n",
        "allFeaturesMissingData = get_missing_data_percentage(train_features)\n",
        "\n",
        "# Drop features with a missing data percentage above the specified threshold\n",
        "train_features = drop_high_missing_data_columns(allFeaturesMissingData, train_features, 70)\n",
        "\n",
        "# Drop features with only 1 distinct value, extremely high or extremely low correlation\n",
        "train_features = drop_one_value_columns(train_features)\n",
        "train_features = drop_high_correlation_features(train_features, 0.80)\n",
        "\n",
        "# Extract the numerical & categorical features from training features\n",
        "numericalFeatures = getNumericalFeatures(train_features)\n",
        "categoricalFeatures = getCategoricalFeatures(train_features)\n",
        "\n",
        "# Get the percentage of missing data for both numerical & categorical features\n",
        "numericalFeaturesMissingData = get_missing_data_percentage(numericalFeatures)\n",
        "categoricalFeaturesMissingData = get_missing_data_percentage(categoricalFeatures)\n",
        "\n",
        "\n",
        "# Impute categorical missing values with \"X\" and numerical missing values with column mean\n",
        "numericalFeatures = numericalFeatures.fillna(numericalFeatures.mean(), inplace=False)\n",
        "categoricalFeatures = categoricalFeatures.fillna(\"X\")\n",
        "\n",
        " #Update missing data and ensure none exists\n",
        "numericalFeaturesMissingData = get_missing_data_percentage(numericalFeatures)\n",
        "categoricalFeaturesMissingData = get_missing_data_percentage(categoricalFeatures)\n",
        "\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt26CGFi5jgE"
      },
      "source": [
        "# [PREPROCESSING STAGE 2] - DATA TRANSFORMATION \n",
        "\n",
        "# Numerically represent the categorical features using label encoding\n",
        "categoricalFeatures = label_encode_categorical_features(categoricalFeatures)\n",
        "\n",
        "# Update training features by replacing the initial data with the imputed data\n",
        "train_features = pd.concat([numericalFeatures, categoricalFeatures], axis=1)\n",
        "\n",
        "# Further split the training data into a train and test sets\n",
        "X_train, Y_train, X_test, Y_test, X_validation, Y_validation = split_data(train_features, train_labels)\n",
        "\n",
        "# Feature Selection using SelectKBest\n",
        "X_train, X_validation, X_test = selectkbestfeatures(X_train, Y_train, X_validation, X_test, 50)\n",
        "\n",
        "# Feature Scaling using Standardization\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_validation = scaler.transform(X_validation)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX_IuzR8Gjz6"
      },
      "source": [
        "# [PREPROCESSING STAGE 3] - DATA REDUCTION (USING PCA or LDA) (focus here next)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=25).fit(X_train)\n",
        "\n",
        "X_train_pca = pca.transform(X_train)\n",
        "X_validation_pca = pca.transform(X_validation)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "X_train = pd.DataFrame(data = X_train_pca)\n",
        "X_validation = pd.DataFrame(data = X_validation_pca)\n",
        "X_test = pd.DataFrame(data = X_test_pca)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "source": [
        "At the moment this takes a very long time so I will avoid it "
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-1-33d6b320d397>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSMOTE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mX_res\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_res\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"# of 1 %d\\n# of 0 %d\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_res\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_res\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ],
      "source": [
        "#SMOTE for dealing with unbalanced data\n",
        "#pip install imblearn\n",
        "import sklearn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import sklearn\n",
        "seed=100 \n",
        "sm=SMOTE(sampling_strategy='auto', k_neighbors=2, random_state=seed)\n",
        "X_res, y_res=sm.fit_resample(X_train,Y_train) \n",
        "\n",
        "print(\"# of 1 %d\\n# of 0 %d\"%(sum(y_res==1),sum(y_res==0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "11073/11073 [==============================] - 42s 4ms/step - loss: 0.0370 - accuracy: 0.7253\n",
            "Epoch 2/10\n",
            "11073/11073 [==============================] - 44s 4ms/step - loss: 0.0352 - accuracy: 0.7386\n",
            "Epoch 3/10\n",
            "11073/11073 [==============================] - 42s 4ms/step - loss: 0.0345 - accuracy: 0.7456\n",
            "Epoch 4/10\n",
            "11073/11073 [==============================] - 42s 4ms/step - loss: 0.0339 - accuracy: 0.7589\n",
            "Epoch 5/10\n",
            "11073/11073 [==============================] - 42s 4ms/step - loss: 0.0333 - accuracy: 0.7611\n",
            "Epoch 6/10\n",
            "11073/11073 [==============================] - 41s 4ms/step - loss: 0.0329 - accuracy: 0.7689\n",
            "Epoch 7/10\n",
            "11073/11073 [==============================] - 41s 4ms/step - loss: 0.0326 - accuracy: 0.7737\n",
            "Epoch 8/10\n",
            "11073/11073 [==============================] - 41s 4ms/step - loss: 0.0324 - accuracy: 0.7716\n",
            "Epoch 9/10\n",
            "11073/11073 [==============================] - 42s 4ms/step - loss: 0.0320 - accuracy: 0.7736\n",
            "Epoch 10/10\n",
            "11073/11073 [==============================] - 41s 4ms/step - loss: 0.0318 - accuracy: 0.7767\n",
            "Total accuracy: 82.12\n",
            "accuracy of 1: 70.97\n",
            "accuracy of 0: 82.52\n"
          ]
        }
      ],
      "source": [
        "#Keras example for getting the NN to work \n",
        "\n",
        "\n",
        "def NNetwork(Tr,A,E,L1,L2,L3,W):#Tr is training set A is answer L1 is layer 1 nodes L2 is layer 2 nodes E is the epochs and W is if weights are needed\n",
        "    NNIn=Tr#take the data and use  new name for it \n",
        "    NNans=A\n",
        "    model_NN=Sequential()\n",
        "\n",
        "    model_NN.add(Dense(L1, input_dim=NNIn.shape[1], activation='elu'))#50 nodes in first hidden layer there might have been an issue with relu but it seems to be fixed\n",
        "    #model_NN.add(Dropout(0.5))\n",
        "    model_NN.add(Dense(L2, activation='elu'))\n",
        "    model_NN.add(Dense(L3, activation='elu'))\n",
        "    #model_NN.add(Dropout(0.5))\n",
        "    #model_NN.add(Dense(L3, activation='relu'))#seems to bring back the all same value issue \n",
        "    model_NN.add(Dense(1, activation='sigmoid'))\n",
        "    model_NN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    if(W==1):#if the weights are needed or not \n",
        "        weights={0:(np.count_nonzero(NNans==1)/NNans.size), 1:(np.count_nonzero(NNans==0)/NNans.size)}#gives more even distribution less loss and not much difference in accuracy \n",
        "        #weights={0:1, 1:27}#increase to 27 gave slightly better results \n",
        "        model_NN.fit(NNIn,NNans,class_weight=weights,epochs=E)# it says 11073 insteead of 354324 because there are 32 batches and 354324/32=11073\n",
        "        \n",
        "    else: \n",
        "        model_NN.fit(NNIn,NNans,epochs=E)\n",
        "    \n",
        "   \n",
        "    return model_NN\n",
        "    NNacc(model_NN,NNIn,NNans)#call the other function to analyze \n",
        "\n",
        "    \n",
        "def NNacc (model, testin,testout):\n",
        "    predictions=model.predict_classes(testin)\n",
        "    check=np.array(testout)\n",
        "    count=0\n",
        "    count1=0\n",
        "    count0=0\n",
        "    for i in range(predictions.size):\n",
        "        if(predictions[i]==check[i]):\n",
        "            count+=1\n",
        "        if( predictions[i]==1 and check[i]==1):\n",
        "            count1+=1\n",
        "        if( predictions[i]==0 and check[i]==0):\n",
        "            count0+=1 \n",
        "    acc=((count/testout.size))*100\n",
        "    print(\"Total accuracy: %.2f\"%(acc))\n",
        "    acc1=(count1/np.count_nonzero(NNans==1))*100\n",
        "    print(\"accuracy of 1: %.2f\"%(acc1))\n",
        "    acc0=(count0/np.count_nonzero(NNans==0))*100\n",
        "    print(\"accuracy of 0: %.2f\"%(acc0))\n",
        "\n",
        "\n",
        "\n",
        "mod=NNetwork(X_train,Y_train,10,100,600,600,1)#train using the X_train and Y_train with epoch and first and second layers and specify 1 if weig\n",
        "NNacc(mod, X_train,Y_train)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEOlx41DGf_c"
      },
      "source": [
        "# [MODEL BUILDING]\n",
        "\n",
        "algorithm = GradientBoostingClassifier()\n",
        "parameters = {\n",
        "    'n_estimators': [5, 50, 250, 500],\n",
        "    'max_depth': [1, 3, 5, 7, 9],\n",
        "    'learning_rate': [0.01, 0.1, 1, 10, 100]\n",
        "}\n",
        "cv = GridSearchCV(algorithm, parameters, cv=5)\n",
        "cv.fit(X_train, Y_train.values.ravel())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-17-bf6920ebbd46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m }\n\u001b[0;32m      9\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[1;32m--> 639\u001b[1;33m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m   1032\u001b[0m         \u001b[1;31m# fit the boosting stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[1;32m-> 1034\u001b[1;33m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[0;32m   1035\u001b[0m         \u001b[1;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1087\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[0;32m   1088\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1089\u001b[1;33m                                      X_csc, X_csr)\n\u001b[0m\u001b[0;32m   1090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m             \u001b[1;31m# track deviance (= loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    786\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m                 tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[1;32m--> 788\u001b[1;33m                          check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m             \u001b[1;31m# update tree leaves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1122\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1125\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugeIK2neHxgy"
      },
      "source": [
        "# [MODEL EVALUATION]\n",
        " \n",
        "evaluate_model('Train Set', cv, X_train, Y_train)\n",
        "evaluate_model('Validation Set', cv, X_validation, Y_validation)\n",
        "evaluate_model('Test Set', cv, X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}