{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "Python 3.6.5 64-bit (conda)",
      "display_name": "Python 3.6.5 64-bit (conda)",
      "metadata": {
        "interpreter": {
          "hash": "e73f067f1c59be12cfda938665c6a960a5d3ec55ce3a05feea7c329cb1f7e9af"
        }
      }
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5-final"
    },
    "colab": {
      "name": "david-fraud-detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "piJFKIoM5jfX"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import collections\n",
        "from time import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn import preprocessing\n",
        "from sklearn import metrics\n",
        "\n",
        "# Models\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "\n",
        "#for the NN\n",
        "from numpy import loadtxt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "#for inbalance issues \n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from collections import Counter\n",
        "from matplotlib import pyplot \n",
        "from imblearn.under_sampling import EditedNearestNeighbours\n",
        "from imblearn.under_sampling import CondensedNearestNeighbour\n",
        "from imblearn.under_sampling import OneSidedSelection\n",
        "from imblearn.under_sampling import NearMiss\n",
        "from imblearn.combine import SMOTEENN"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFoUe-Ed5jfg"
      },
      "source": [
        "def prepare_inputs_and_outputs(data):\n",
        "    \n",
        "    # Prepare & save the inputs and outputs features\n",
        "    features = data.drop(['isFraud','TransactionID'], axis = 1)\n",
        "    labels = data[['isFraud']]\n",
        "    \n",
        "    return features, labels"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGBJ2vBk5jfj"
      },
      "source": [
        "def get_missing_data_percentage(data):\n",
        "    \n",
        "    # where mvp = missing value percentages\n",
        "    mvp = data.isnull().sum() * 100 / len(data)\n",
        "    mvp = pd.DataFrame({'Feature': data.columns,'Percentage': mvp})\n",
        "    \n",
        "    return mvp.sort_values(by ='Percentage', ascending=False)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCxN6o7n5jfn"
      },
      "source": [
        "def drop_high_missing_data_columns(mvd, data, threshold):\n",
        "    # Where \"mvd\" = missing value data\n",
        "    # Get names of indexes for which column missing data is over 50%\n",
        "    high_missing_data_cols = mvd[mvd['Percentage'] > threshold].index\n",
        "\n",
        "    for col_name in range(len(high_missing_data_cols)):\n",
        "        del data[high_missing_data_cols[col_name]] # Delete rows from dataFrame??? or columns\n",
        "    \n",
        "    return data"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQoerlZK5jfr"
      },
      "source": [
        "def drop_one_value_columns(data):\n",
        "    \n",
        "    # Drop columns with only 1 unique value.\n",
        "    for column in data.columns:\n",
        "        if len(data[column].unique()) == 1:\n",
        "            #print(traindata[column].name)\n",
        "            data.drop(column,inplace=True,axis=1)\n",
        "            \n",
        "    return data"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPIc725i5jfx"
      },
      "source": [
        "def getCategoricalFeatures(data):\n",
        "    columns = list(data)\n",
        "    result = []\n",
        "    for c in columns:\n",
        "        if data.dtypes[c] == np.object:\n",
        "            result.append(c)\n",
        "    return data[result]\n",
        "\n",
        "def getNumericalFeatures(data):\n",
        "    columns = list(data)\n",
        "    result = []\n",
        "    for c in columns: \n",
        "        if data.dtypes[c] != np.object:\n",
        "            result.append(c) \n",
        "    return data[result]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3hxDcdW5jf0"
      },
      "source": [
        "def drop_high_correlation_features(data, threshold):\n",
        "\n",
        "    corr_matrix = data.corr()\n",
        "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "    to_drop = [column for column in upper.columns if any(abs(upper[column]) > threshold)]\n",
        "    data = data.drop(columns = to_drop)\n",
        "    \n",
        "    return data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSE1yVrx5jf4"
      },
      "source": [
        "def label_encode_categorical_features(data):\n",
        "    encoder_dict = collections.defaultdict(LabelEncoder)\n",
        "    data = data.apply(lambda x: encoder_dict[x.name].fit_transform(x))\n",
        "    return data"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE1lS-qa5jf8"
      },
      "source": [
        "def split_data(features, labels):\n",
        "    \n",
        "    # Data Splitting: 60% for training, 20% for validation and 20% for testing.\n",
        "    X_train, X_test, Y_train, y_test = train_test_split(features, labels, test_size=0.4)\n",
        "    X_validation, X_test, Y_validation, y_test = train_test_split(X_test, y_test, test_size=0.5)\n",
        "    \n",
        "    return X_train, Y_train, X_test, y_test, X_validation, Y_validation"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDfUoKCN5jf_"
      },
      "source": [
        "def selectkbestfeatures(X_train, Y_train, X_validation, X_test, numberOfFeatures):\n",
        "\n",
        "    fit = SelectKBest(score_func=f_classif, k=numberOfFeatures).fit(X_train, Y_train)\n",
        "\n",
        "    X_train = fit.transform(X_train)\n",
        "    X_validation = fit.transform(X_validation)\n",
        "    X_test = fit.transform(X_test)\n",
        "\n",
        "    # Get column names from the best features\n",
        "    X_train_cols = fit.get_support(indices=True)\n",
        "    X_validation_cols = fit.get_support(indices=True)\n",
        "    X_test_cols = fit.get_support(indices=True)\n",
        "\n",
        "    X_train = pd.DataFrame(X_train, columns=X_train_cols)\n",
        "    X_validation = pd.DataFrame(X_validation, columns=X_validation_cols)\n",
        "    X_test = pd.DataFrame(X_test, columns=X_test_cols)\n",
        "\n",
        "    # Create new dataframes with the column names\n",
        "    #X_train = X_train.iloc[:,X_train_cols]\n",
        "    #X_validation = X_validation.iloc[:,X_validation_cols]\n",
        "    #X_test = X_test.iloc[:,X_test_cols]\n",
        "\n",
        "    return X_train, X_validation, X_test"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DXZ6J2a5jgB"
      },
      "source": [
        "def evaluate_model(name, model, features, labels):\n",
        "    \n",
        "    start = time()\n",
        "    pred = model.predict(features)\n",
        "    end = time()\n",
        "    \n",
        "    # Print the confusion matrix\n",
        "    print(metrics.confusion_matrix(labels, pred))\n",
        "\n",
        "    # Print the precision and recall, among other metrics\n",
        "    print(metrics.classification_report(labels, pred, digits=3))\n",
        "    \n",
        "    print(name+\" Accuracy - \"+str(round(accuracy_score(labels, pred), 3) * 100)+\"%\")\n",
        "    print(name+\" Precision - \"+str(round(precision_score(labels, pred, average='micro'), 3) * 100)+\"%\")\n",
        "    print(name+\" Recall - \"+str(round(recall_score(labels, pred, average='micro'), 3) * 100)+\"%\")\n",
        "    print(name+\" F1 Score - \"+str(round(f1_score(labels, pred, average='micro'), 3) * 100)+\"%\")\n",
        "    print(name+\" Latency - \"+str(round((end - start) * 1000, 1))+\"ms \\n\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "source": [
        "This will be the start of the data analytics and will make use of the above functions "
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# read and combine train data by the TransactionID\n",
        "#this has been changed to read from the local machine instad of from google drive as is done with colab \n",
        "train_identity = pd.read_csv(\"D:\\School\\Fifth Year\\Large Scale Data Analytics\\Project\\ieee-fraud-detection\\\\train_identity.csv\")\n",
        "train_transaction = pd.read_csv(\"D:\\School\\Fifth Year\\Large Scale Data Analytics\\Project\\ieee-fraud-detection\\\\train_transaction.csv\")\n",
        "#combine the data so we can go through the whole thing \n",
        "traindata = pd.merge(train_transaction,train_identity, on='TransactionID', how='left',left_index=True,right_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxwvqD1k7R1Q"
      },
      "source": [
        "# Separate Features & Labels\n",
        "train_features, train_labels = prepare_inputs_and_outputs(traindata)#used to be at the top but I moved it in order to have the"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANvotlOt7di1"
      },
      "source": [
        "# [PREPROCESSING STAGE 1] - DATA CLEANING\n",
        "\n",
        "# Examine the percentage of missing data for all feature in the training data\n",
        "allFeaturesMissingData = get_missing_data_percentage(train_features)\n",
        "\n",
        "# Drop features with a missing data percentage above the specified threshold\n",
        "train_features = drop_high_missing_data_columns(allFeaturesMissingData, train_features, 70)\n",
        "\n",
        "# Drop features with only 1 distinct value, extremely high or extremely low correlation\n",
        "train_features = drop_one_value_columns(train_features)\n",
        "train_features = drop_high_correlation_features(train_features, 0.80)\n",
        "\n",
        "# Extract the numerical & categorical features from training features\n",
        "numericalFeatures = getNumericalFeatures(train_features)\n",
        "categoricalFeatures = getCategoricalFeatures(train_features)\n",
        "\n",
        "# Get the percentage of missing data for both numerical & categorical features\n",
        "numericalFeaturesMissingData = get_missing_data_percentage(numericalFeatures)\n",
        "categoricalFeaturesMissingData = get_missing_data_percentage(categoricalFeatures)\n",
        "\n",
        "\n",
        "# Impute categorical missing values with \"X\" and numerical missing values with column mean\n",
        "numericalFeatures = numericalFeatures.fillna(numericalFeatures.mean(), inplace=False)\n",
        "categoricalFeatures = categoricalFeatures.fillna(\"X\")\n",
        "\n",
        " #Update missing data and ensure none exists\n",
        "numericalFeaturesMissingData = get_missing_data_percentage(numericalFeatures)\n",
        "categoricalFeaturesMissingData = get_missing_data_percentage(categoricalFeatures)\n",
        "\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt26CGFi5jgE"
      },
      "source": [
        "# [PREPROCESSING STAGE 2] - DATA TRANSFORMATION \n",
        "\n",
        "# Numerically represent the categorical features using label encoding\n",
        "categoricalFeatures = label_encode_categorical_features(categoricalFeatures)\n",
        "\n",
        "# Update training features by replacing the initial data with the imputed data\n",
        "train_features = pd.concat([numericalFeatures, categoricalFeatures], axis=1)\n",
        "\n",
        "# Further split the training data into a train and test sets\n",
        "X_train, Y_train, X_test, Y_test, X_validation, Y_validation = split_data(train_features, train_labels)\n",
        "\n",
        "# Feature Selection using SelectKBest\n",
        "X_train, X_validation, X_test = selectkbestfeatures(X_train, Y_train, X_validation, X_test, 50)\n",
        "\n",
        "# Feature Scaling using Standardization\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_validation = scaler.transform(X_validation)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX_IuzR8Gjz6"
      },
      "source": [
        "# [PREPROCESSING STAGE 3] - DATA REDUCTION (USING PCA or LDA) (focus here next)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=25).fit(X_train)\n",
        "\n",
        "X_train_pca = pca.transform(X_train)\n",
        "X_validation_pca = pca.transform(X_validation)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "X_train = pd.DataFrame(data = X_train_pca)\n",
        "X_validation = pd.DataFrame(data = X_validation_pca)\n",
        "X_test = pd.DataFrame(data = X_test_pca)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "source": [
        "At the moment this takes a very long time so I will avoid it "
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "#SMOTE for dealing with unbalanced data\n",
        "#pip install imblearn\n",
        "########################################################### different methods to balance data\n",
        "from imblearn.combine import SMOTETomek\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "\n",
        "def smote(K,x,y):\n",
        "    seed=100 \n",
        "    sm=SMOTE(sampling_strategy='auto', k_neighbors=K, random_state=seed)\n",
        "    #sm=SMOTE()\n",
        "    X_res, y_res=sm.fit_resample(x,y)\n",
        "\n",
        "    #print(\"SMOTE\\n# of 1 %d\\n# of 0 %d\"%(np.count_nonzero(y_res==1),np.count_nonzero(y_res==0)))\n",
        "    return X_res, y_res\n",
        "\n",
        "def ada(x,y):\n",
        "    adas=ADASYN()\n",
        "    c_res,d_res=adas.fit_resample(x,y)\n",
        "    #print(\"ADASYN\\n# of 1 %d\\n# of 0 %d\"%(np.count_nonzero(d_res==1),np.count_nonzero(d_res==0)))\n",
        "    return c_res, d_res\n",
        "\n",
        "def ENN(K, x,y):\n",
        "    undersample= EditedNearestNeighbours(n_neighbors=K)\n",
        "    a,b=undersample.fit_resample(x,y)\n",
        "    return a, b\n",
        "\n",
        "def CNN(K,x,y):#relativly slow so do not use again \n",
        "    undersample=CondensedNearestNeighbour(n_neighbors=K)\n",
        "    X,Y=undersample.fit_resample(x,y)\n",
        "    return X,Y\n",
        "\n",
        "def OSS(n,s,x,y):# possibly slow because it uses CNN\n",
        "    undersample=OneSidedSelection(n_neighbors=n, n_seeds_S=s)\n",
        "    X,Y=undersample.fit_resample(x,y)\n",
        "    return X,Y\n",
        "\n",
        "def NM(n,x,y):# DO NOT USE v 2 TOOK UP 54G of RAM\n",
        "    undersample=NearMiss(version=3,n_neighbors_ver3=n)\n",
        "    X,Y=undersample.fit_resample(x,y)\n",
        "    return X,Y\n",
        "def comb(x,y):\n",
        "    #resample = SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))\n",
        "    resample = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))\n",
        "    retx,rety=resample.fit_resample(x,y)\n",
        "    return retx, rety\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined\n",
            "Epoch 1/50\n",
            "21355/21355 [==============================] - 64s 3ms/step - loss: 0.4559 - accuracy: 0.7733\n",
            "Epoch 2/50\n",
            "21355/21355 [==============================] - 64s 3ms/step - loss: 0.3896 - accuracy: 0.8068\n",
            "Epoch 3/50\n",
            "21355/21355 [==============================] - 65s 3ms/step - loss: 0.3670 - accuracy: 0.8178\n",
            "Epoch 4/50\n",
            " 3218/21355 [===>..........................] - ETA: 56s - loss: 0.3577 - accuracy: 0.8226"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-58-d8644fceda89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;31m##########################################################################most results tracked in NN Testing doc on google drive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Combined\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m \u001b[0mmod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m120\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m212\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m320\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"With balanced training data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[0mNNacc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-58-d8644fceda89>\u001b[0m in \u001b[0;36mNNetwork\u001b[1;34m(Tr, A, E, L1, L2, L3, L4, W)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mmodel_NN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNNIn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNNans\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#Keras example for getting the NN to work \n",
        "\n",
        "\n",
        "def NNetwork(Tr,A,E,L1,L2,L3,L4,W):#Tr is training set A is answer L1 is layer 1 nodes L2 is layer 2 nodes E is the epochs and W is if weights are needed\n",
        "    NNIn=Tr#take the data and use  new name for it \n",
        "    NNans=A\n",
        "    ##########################################layers are added and removed here to test different implementations\n",
        "    model_NN=Sequential()\n",
        "    model_NN.add(Dense(L1, input_dim=NNIn.shape[1], activation='relu'))#50 nodes in first hidden layer there might have been an issue with relu but it seems to be fixed\n",
        "    model_NN.add(Dropout(0.1))\n",
        "    model_NN.add(Dense(L2, activation='relu'))\n",
        "    #model_NN.add(Dense(L3, activation='relu',kernel_initializer='uniform'))\n",
        "    model_NN.add(Dropout(0.1))\n",
        "    model_NN.add(Dense(L3, activation='relu'))#seems to bring back the all same value issue \n",
        "    model_NN.add(Dropout(0.1))\n",
        "    model_NN.add(Dense(1, activation='sigmoid',kernel_initializer='uniform'))\n",
        "    model_NN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    if(W==1):#if the weights are needed or not \n",
        "        weights={0:(np.count_nonzero(NNans==1)/NNans.size), 1:(np.count_nonzero(NNans==0)/NNans.size)}#gives more even distribution less loss and not much difference in accuracy \n",
        "        #weights={0:7, 1:193}#increase to 27 gave slightly better results \n",
        "        model_NN.fit(NNIn,NNans,class_weight=weights,epochs=E)# it says 11073 insteead of 354324 because there are 32 batches and 354324/32=11073\n",
        "        \n",
        "    else: \n",
        "        model_NN.fit(NNIn,NNans,epochs=E)\n",
        "    \n",
        "   \n",
        "    return model_NN\n",
        "    NNacc(model_NN,NNIn,NNans)#call the other function to analyze \n",
        "\n",
        "    \n",
        "def NNacc (model, testin,testout):\n",
        "    predictions=model.predict_classes(testin)\n",
        "    check=np.array(testout)\n",
        "    count=0\n",
        "    count1=0\n",
        "    count0=0\n",
        "    for i in range(predictions.size):\n",
        "        if(predictions[i]==check[i]):\n",
        "            count+=1\n",
        "        if( predictions[i]==1 and check[i]==1):\n",
        "            count1+=1\n",
        "        if( predictions[i]==0 and check[i]==0):\n",
        "            count0+=1 \n",
        "    acc=((count/testout.size))*100\n",
        "    acc1=(count1/np.count_nonzero(check==1))*100\n",
        "    acc0=(count0/np.count_nonzero(check==0))*100\n",
        "    print(\"Total accuracy: %d    Accuracy of 1: %d     Accuracy of 0: %d\"%(round(acc),round(acc1),round(acc0)))\n",
        "\n",
        "########################################### collection of different function calls to balance the data  \n",
        "#bal_x, bal_y =smote(3,X_train,Y_train)#get more balanced data using smote with the input and output as well as the K value \n",
        "\n",
        "#ad_x,ad_y=ada(X_train,Y_train)#get more balanced data using ADASYN\n",
        "\n",
        "#u_x,u_y=ENN(12,X_train,Y_train)\n",
        "\n",
        "#CNN_x,CNN_y=CNN(1,X_train,Y_train)#relativly slow do not use again \n",
        "#print(\"Done ENN\")#takes a fair amount of time \n",
        "#NM_x, NM_y=NM(3,X_train,Y_train)#try this one if they still take a long time \n",
        "#print(\"Done NM\")#does not take long but does when using second type\n",
        "#ONN_x,ONN_y=OSS(4,200,X_train, Y_train)\n",
        "#print(\"Done ENN\")\n",
        "#combination \n",
        "\n",
        "cx,cy=comb(X_train,Y_train)\n",
        "#mod=NNetwork(X_train,Y_train,10,100,600,250,1)#train using the X_train and Y_train with epoch and first and second layers and specify 1 if weig\n",
        "\n",
        "#####################################################different NN implimentations and printouts based on each data for comperison\n",
        "\"\"\" print(\"SMOTE\")\n",
        "mod=NNetwork(bal_x,bal_y,10,500,1000,280,400,0)#train using the X_train and Y_train with epoch and first and second layers and specify 1 if weig\n",
        "print(\"With balanced training data\")\n",
        "NNacc(mod, bal_x,bal_y)\n",
        "print(\"With origional training data\")\n",
        "NNacc(mod, X_train,Y_train)\n",
        "print(\"with origional testing data\")\n",
        "NNacc(mod, X_test, Y_test)\n",
        "\n",
        "\n",
        "print(\"ADASYN\")\n",
        "mod=NNetwork(ad_x,ad_y,10,500,1000,280,400,0)#train using the X_train and Y_train with epoch and first and second layers and specify 1 if weig\n",
        "print(\"With balanced training data\")\n",
        "NNacc(mod, ad_x,ad_y)\n",
        "print(\"With origional training data\")\n",
        "NNacc(mod, X_train,Y_train)\n",
        "print(\"with origional testing data\")\n",
        "NNacc(mod, X_test, Y_test) \n",
        "  \n",
        "\n",
        "print(\"Edited NN\")\n",
        "mod=NNetwork(u_x,u_y,10,100,100,280,400,0)#\n",
        "print(\"With balanced training data\")\n",
        "NNacc(mod, u_x,u_y)\n",
        "print(\"With origional training data\")\n",
        "NNacc(mod, X_train,Y_train)\n",
        "print(\"with origional testing data\")\n",
        "NNacc(mod, X_test, Y_test) \n",
        "\n",
        "\n",
        "print(\"One Sided NN\")\n",
        "mod=NNetwork(ONN_x,ONN_y,10,100,100,280,400,0)#\n",
        "print(\"With balanced training data\")\n",
        "NNacc(mod, ONN_x,ONN_y)\n",
        "print(\"With origional training data\")\n",
        "NNacc(mod, X_train,Y_train)\n",
        "print(\"with origional testing data\")\n",
        "NNacc(mod, X_test, Y_test) \n",
        "\n",
        "\n",
        "print(\"Near Miss\")\n",
        "mod=NNetwork(NM_x,NM_y,10,100,600,300,400,0)#\n",
        "print(\"With balanced training data\")\n",
        "NNacc(mod, NM_x,NM_y)\n",
        "print(\"With origional training data\")\n",
        "NNacc(mod, X_train,Y_train)\n",
        "print(\"with origional testing data\")\n",
        "NNacc(mod, X_test, Y_test) \n",
        "\"\"\"\n",
        "##########################################################################most results tracked in NN Testing doc on google drive \n",
        "print(\"Combined\")\n",
        "mod=NNetwork(cx,cy,50,120,212,320,400,0)#\n",
        "print(\"With balanced training data\")\n",
        "NNacc(mod, cx,cy)\n",
        "print(\"With origional training data\")\n",
        "NNacc(mod, X_train,Y_train)\n",
        "print(\"with origional testing data\")\n",
        "NNacc(mod, X_test, Y_test) \n",
        "\n",
        "\"\"\" print(\"NN Added Weights\")\n",
        "mod=NNetwork(X_train,Y_train,10,100,100,280,400,1)#train using the X_train and Y_train with epoch and first and second layers and specify 1 if weig\n",
        "print(\"With balanced training data\")\n",
        "NNacc(mod, X_train,Y_train)\n",
        "print(\"with origional testing data\")\n",
        "NNacc(mod, X_test, Y_test) \"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEOlx41DGf_c"
      },
      "source": [
        "# [MODEL BUILDING]\n",
        "\n",
        "algorithm = GradientBoostingClassifier()\n",
        "parameters = {\n",
        "    'n_estimators': [5, 50, 250, 500],\n",
        "    'max_depth': [1, 3, 5, 7, 9],\n",
        "    'learning_rate': [0.01, 0.1, 1, 10, 100]\n",
        "}\n",
        "cv = GridSearchCV(algorithm, parameters, cv=5)\n",
        "cv.fit(X_train, Y_train.values.ravel())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-17-bf6920ebbd46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m }\n\u001b[0;32m      9\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[1;32m--> 639\u001b[1;33m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m   1032\u001b[0m         \u001b[1;31m# fit the boosting stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[1;32m-> 1034\u001b[1;33m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[0;32m   1035\u001b[0m         \u001b[1;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1087\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[0;32m   1088\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1089\u001b[1;33m                                      X_csc, X_csr)\n\u001b[0m\u001b[0;32m   1090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m             \u001b[1;31m# track deviance (= loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    786\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m                 tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[1;32m--> 788\u001b[1;33m                          check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m             \u001b[1;31m# update tree leaves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1122\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1125\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugeIK2neHxgy"
      },
      "source": [
        "# [MODEL EVALUATION]\n",
        " \n",
        "evaluate_model('Train Set', cv, X_train, Y_train)\n",
        "evaluate_model('Validation Set', cv, X_validation, Y_validation)\n",
        "evaluate_model('Test Set', cv, X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}