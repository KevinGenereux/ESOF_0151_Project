{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"EDA.ipynb","provenance":[],"authorship_tag":"ABX9TyMbXhQfnHRaPf5Q8khuwloU"},"kernelspec":{"name":"Python 3.6.5 64-bit ('Shared': virtualenv)","display_name":"Python 3.6.5 64-bit ('Shared': virtualenv)","metadata":{"interpreter":{"hash":"e73f067f1c59be12cfda938665c6a960a5d3ec55ce3a05feea7c329cb1f7e9af"}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"WDTO9kafz1CK"},"source":["***Importing Libraries***"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","\n","#for the NN\n","from numpy import loadtxt\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras import backend as K\n","import tensorflow as tf\n","\n","#for inbalance issues \n","from imblearn.over_sampling import SMOTE\n","from imblearn.over_sampling import ADASYN\n","from collections import Counter\n","from matplotlib import pyplot \n","from imblearn.under_sampling import EditedNearestNeighbours\n","from imblearn.under_sampling import CondensedNearestNeighbour\n","from imblearn.under_sampling import OneSidedSelection\n","from imblearn.under_sampling import NearMiss\n","from imblearn.combine import SMOTEENN"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["#SMOTE for dealing with unbalanced data\n","#pip install imblearn\n","########################################################### different methods to balance data\n","from imblearn.combine import SMOTETomek\n","from imblearn.under_sampling import TomekLinks\n","\n","def smote(K,x,y):\n","    seed=100 \n","    sm=SMOTE(sampling_strategy='auto', k_neighbors=K, random_state=seed)\n","    #sm=SMOTE()\n","    X_res, y_res=sm.fit_resample(x,y)\n","\n","    #print(\"SMOTE\\n# of 1 %d\\n# of 0 %d\"%(np.count_nonzero(y_res==1),np.count_nonzero(y_res==0)))\n","    return X_res, y_res\n","\n","def ada(x,y):\n","    adas=ADASYN()\n","    c_res,d_res=adas.fit_resample(x,y)\n","    #print(\"ADASYN\\n# of 1 %d\\n# of 0 %d\"%(np.count_nonzero(d_res==1),np.count_nonzero(d_res==0)))\n","    return c_res, d_res\n","\n","def ENN(K, x,y):\n","    undersample= EditedNearestNeighbours(n_neighbors=K)\n","    a,b=undersample.fit_resample(x,y)\n","    return a, b\n","\n","def CNN(K,x,y):#relativly slow so do not use again \n","    undersample=CondensedNearestNeighbour(n_neighbors=K)\n","    X,Y=undersample.fit_resample(x,y)\n","    return X,Y\n","\n","def OSS(n,s,x,y):# possibly slow because it uses CNN\n","    undersample=OneSidedSelection(n_neighbors=n, n_seeds_S=s)\n","    X,Y=undersample.fit_resample(x,y)\n","    return X,Y\n","\n","def NM(n,x,y):# DO NOT USE v 2 TOOK UP 54G of RAM\n","    undersample=NearMiss(version=3,n_neighbors_ver3=n)\n","    X,Y=undersample.fit_resample(x,y)\n","    return X,Y\n","def comb(x,y):\n","    #resample = SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))\n","    resample = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))\n","    retx,rety=resample.fit_resample(x,y)\n","    return retx, rety"]},{"cell_type":"code","metadata":{"id":"Zo7499G17Qk8"},"source":["import pandas as pd\n","import numpy as np\n","from ReadDataset import readFiles\n","\n","\n","def reduce_mem_usage(df):\n","    start_mem = df.memory_usage().sum() / 1024**2\n","    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n","    \n","    for col in df.columns:\n","        col_type = df[col].dtype\n","        \n","        if col_type != object:\n","            if str(col_type)[:3] == 'int':\n","                if df[col].max() < 2 ** 7:\n","                    df[col] = df[col].astype(np.int8)\n","                elif df[col].max() < 2 ** 15:\n","                    df[col] = df[col].astype(np.int16)\n","                elif df[col].max() < 2 ** 31:\n","                    df[col] = df[col].astype(np.int32)\n","                else:\n","                    df[col] = df[col].astype(np.int64)  \n","            else:\n","                if df[col].max() < np.finfo(np.float16).max:\n","                    df[col] = df[col].astype(np.float16)\n","                elif df[col].max() < np.finfo(np.float32).max:\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float64)\n","        else:\n","            df[col] = df[col].astype('category')\n","\n","    end_mem = df.memory_usage().sum() / 1024**2\n","    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n","    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n","    \n","    return df\n","\n","def simplifyColumnValues(df):\n","    df.loc[df['DeviceInfo'].str.contains('SM', na=False), 'DeviceInfo'] = 'Samsung'\n","    df.loc[df['DeviceInfo'].str.contains('SAMSUNG', na=False), 'DeviceInfo'] = 'Samsung'\n","    df.loc[df['DeviceInfo'].str.contains('Moto', na=False), 'DeviceInfo'] = 'Motorola'\n","    df.loc[df['DeviceInfo'].str.contains('moto', na=False), 'DeviceInfo'] = 'Motorola'\n","    df.loc[df['DeviceInfo'].str.contains('LG-', na=False), 'DeviceInfo'] = 'LG'\n","    df.loc[df['DeviceInfo'].str.contains('rv:', na=False), 'DeviceInfo'] = 'RV'\n","    df.loc[df['DeviceInfo'].str.contains('HUAWEI', na=False), 'DeviceInfo'] = 'Huawei'\n","    df.loc[df['DeviceInfo'].str.contains('Blade', na=False), 'DeviceInfo'] = 'ZTE'\n","    df.loc[df['DeviceInfo'].str.contains('BLADE', na=False), 'DeviceInfo'] = 'ZTE'\n","    df.loc[df['DeviceInfo'].str.contains('Linux', na=False), 'DeviceInfo'] = 'Linux'\n","    df.loc[df['DeviceInfo'].str.contains('ASUS', na=False), 'DeviceInfo'] = 'Asus'\n","    return df\n","\n","def generateNewFeatures(train, test):\n","    # New feature - decimal part of the transaction amount.\n","    train['TransactionAmt_decimal'] = ((train['TransactionAmt'] - train['TransactionAmt'].astype(int)) * 1000).astype(int)\n","    test['TransactionAmt_decimal'] = ((test['TransactionAmt'] - test['TransactionAmt'].astype(int)) * 1000).astype(int)\n","\n","    # New feature - day of week in which a transaction happened.\n","    train['Transaction_day_of_week'] = np.floor((train['TransactionDT'] / (3600 * 24) - 1) % 7)\n","    test['Transaction_day_of_week'] = np.floor((test['TransactionDT'] / (3600 * 24) - 1) % 7)\n","\n","    # New feature - hour of the day in which a transaction happened.\n","    train['Transaction_hour'] = np.floor(train['TransactionDT'] / 3600) % 24\n","    test['Transaction_hour'] = np.floor(test['TransactionDT'] / 3600) % 24\n","\n","    return train, test\n","\n","# simply replace missing values using -9999\n","# don't want to use techniques like mean, median, and mode as they will make the fraud transaction too similar to the non-fraud transactions\n","def replaceMissingValues(df):\n","    for col in df.columns:\n","        if df[col].dtype.name=='category':\n","            df[col]=df[col].add_categories(\"-x\")\n","            df[col].fillna('-x')\n","        else:\n","            df[col].fillna(-9999)\n","    return df\n","\n","# dimensionality reduction: removing features with less importance\n","def removeFeatures(train, test):\n","    # useful features extracted from Principal Component Analysis\n","    useful_features = ['TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1',\n","                       'P_emaildomain', 'R_emaildomain', 'C1', 'C2', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13',\n","                       'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M2', 'M3',\n","                       'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V17',\n","                       'V19', 'V20', 'V29', 'V30', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V40', 'V44', 'V45', 'V46', 'V47', 'V48',\n","                       'V49', 'V51', 'V52', 'V53', 'V54', 'V56', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V69', 'V70', 'V71',\n","                       'V72', 'V73', 'V74', 'V75', 'V76', 'V78', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V87', 'V90', 'V91', 'V92',\n","                       'V93', 'V94', 'V95', 'V96', 'V97', 'V99', 'V100', 'V126', 'V127', 'V128', 'V130', 'V131', 'V138', 'V139', 'V140',\n","                       'V143', 'V145', 'V146', 'V147', 'V149', 'V150', 'V151', 'V152', 'V154', 'V156', 'V158', 'V159', 'V160', 'V161',\n","                       'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V169', 'V170', 'V171', 'V172', 'V173', 'V175', 'V176', 'V177',\n","                       'V178', 'V180', 'V182', 'V184', 'V187', 'V188', 'V189', 'V195', 'V197', 'V200', 'V201', 'V202', 'V203', 'V204',\n","                       'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V219', 'V220',\n","                       'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V231', 'V233', 'V234', 'V238', 'V239',\n","                       'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V249', 'V251', 'V253', 'V256', 'V257', 'V258', 'V259', 'V261',\n","                       'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276',\n","                       'V277', 'V278', 'V279', 'V280', 'V282', 'V283', 'V285', 'V287', 'V288', 'V289', 'V291', 'V292', 'V294', 'V303',\n","                       'V304', 'V306', 'V307', 'V308', 'V310', 'V312', 'V313', 'V314', 'V315', 'V317', 'V322', 'V323', 'V324', 'V326',\n","                       'V329', 'V331', 'V332', 'V333', 'V335', 'V336', 'V338', 'id_01', 'id_02', 'id_03', 'id_05', 'id_06', 'id_09',\n","                       'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_17', 'id_19', 'id_20', 'id_30', 'id_31', 'id_32', 'id_33',\n","                       'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'device_name', 'device_version', 'OS_id_30', 'version_id_30',\n","                       'browser_id_31', 'version_id_31', 'screen_width', 'screen_height', 'had_id']\n","    \n","    # determining columns not found in useful features list\n","    cols_to_drop = [col for col in train.columns if col not in useful_features]\n","    cols_to_drop.remove('isFraud')\n","    cols_to_drop.remove('TransactionDT')\n","\n","    # removing columns from both train and test\n","    train = train.drop(cols_to_drop, axis=1)\n","    test = test.drop(cols_to_drop, axis=1)\n","    \n","    return train, test\n","\n","def makeNumeric(dat):\n","    enc=LabelEncoder()# create the label encoder\n","    for cols in dat:#loop through all of the columns\n","        if dat[cols].dtype.name==\"object\":#if the data is in a string format we will need to convert it to numeric to find the correlation \n","            enc.fit(dat[cols].astype(str))#fit the column to the encoder to convert to numeric \n","            dat[cols]=enc.fit_transform(dat[cols].astype(str))#transform the data into numeric \n","    return dat\n","\n","def label_encode_categorical_features(data):\n","    encoder_dict = collections.defaultdict(LabelEncoder)\n","    for cols in data:\n","        if data[cols].dtype.name==\"object\":\n","            data[cols] = data[cols].apply(lambda x: encoder_dict[x.name].fit_transform(x))\n","    return data"],"execution_count":23,"outputs":[]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Memory usage of dataframe is 1959.88 MB\n","Memory usage after optimization is: 527.82 MB\n","Decreased by 73.1%\n"]},{"output_type":"error","ename":"ValueError","evalue":"cannot index with vector containing NA / NaN values","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32m<ipython-input-25-36b19cdcd137>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mfull\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mfull\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreduce_mem_usage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mfull\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msimplifyColumnValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32m<ipython-input-23-1d46c8c7942c>\u001b[0m in \u001b[0;36msimplifyColumnValues\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msimplifyColumnValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DeviceInfo'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'SM'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'DeviceInfo'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Samsung'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DeviceInfo'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'SAMSUNG'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'DeviceInfo'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Samsung'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DeviceInfo'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Moto'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'DeviceInfo'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Motorola'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_setitem_indexer\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_setter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_tuple\u001b[1;34m(self, key, is_setter)\u001b[0m\n\u001b[0;32m    245\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Too many indexers'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m                 \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_setter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_setter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m                 \u001b[0mkeyidx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyidx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[0;32m   1297\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1299\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1300\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1301\u001b[0m                 \u001b[0minds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\pandas\\core\\common.py\u001b[0m in \u001b[0;36mis_bool_indexer\u001b[1;34m(key)\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_bool_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m                     raise ValueError('cannot index with vector containing '\n\u001b[0m\u001b[0;32m    105\u001b[0m                                      'NA / NaN values')\n\u001b[0;32m    106\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mValueError\u001b[0m: cannot index with vector containing NA / NaN values"]}],"source":["train_identity = pd.read_csv(\"D:\\School\\Fifth Year\\Large Scale Data Analytics\\Project\\ieee-fraud-detection\\\\train_identity.csv\")\n","train_transaction = pd.read_csv(\"D:\\School\\Fifth Year\\Large Scale Data Analytics\\Project\\ieee-fraud-detection\\\\train_transaction.csv\")\n","\n","full=train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\n","full=replaceMissingValues(full)\n","full=pd.DataFrame(data=full)\n","full=reduce_mem_usage(full)\n","full=simplifyColumnValues(full)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Keras example for getting the NN to work \n","from keras.optimizers import SGD, RMSprop\n","from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_curve, auc, roc_auc_score\n","sgd=SGD(lr=0.1)\n","def NNetwork(Tr,A,E,L1,L2,L3,L4,W):#Tr is training set A is answer L1 is layer 1 nodes L2 is layer 2 nodes E is the epochs and W is if weights are needed\n","    NNIn=Tr#take the data and use  new name for it \n","    NNans=A\n","    ##########################################layers are added and removed here to test different implementations\n","    model_NN=Sequential()\n","    model_NN.add(Dense(L1, input_dim=NNIn.shape[1], activation='relu'))#50 nodes in first hidden layer there might have been an issue with relu but it seems to be fixed\n","    model_NN.add(Dropout(0.1))\n","    model_NN.add(Dense(L2, activation='relu'))\n","    #model_NN.add(Dense(L3, activation='relu',kernel_initializer='uniform'))\n","    \n","    #model_NN.add(Dense(L3, activation='relu'))#seems to bring back the all same value issue \n","    #model_NN.add(Dropout(0.1))\n","    model_NN.add(Dense(1, activation='sigmoid'))\n","    model_NN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    #model_NN.compile(optimizer=sgd,loss='mse')##################################################try another optimizer\n","    if(W==1):#if the weights are needed or not \n","        weights={0:(np.count_nonzero(NNans==1)/NNans.size), 1:(np.count_nonzero(NNans==0)/NNans.size)}#gives more even distribution less loss and not much difference in accuracy \n","        #weights={0:7, 1:193}#increase to 27 gave slightly better results \n","        model_NN.fit(NNIn,NNans,class_weight=weights,epochs=E)# it says 11073 insteead of 354324 because there are 32 batches and 354324/32=11073\n","        \n","    else: \n","        model_NN.fit(NNIn,NNans,epochs=E)\n","    \n","   \n","    return model_NN\n","    NNacc(model_NN,NNIn,NNans)#call the other function to analyze \n","\n","    \n","def NNacc (model, testin,testout):\n","    predictions=model.predict_classes(testin)\n","    check=np.array(testout)\n","    count=0\n","    count1=0\n","    count0=0\n","    for i in range(predictions.size):\n","        if(predictions[i]==check[i]):\n","            count+=1\n","        if( predictions[i]==1 and check[i]==1):\n","            count1+=1\n","        if( predictions[i]==0 and check[i]==0):\n","            count0+=1 \n","    acc=((count/testout.size))*100\n","    acc1=(count1/np.count_nonzero(check==1))*100\n","    acc0=(count0/np.count_nonzero(check==0))*100\n","    print(\"Total accuracy: %d    Accuracy of 1: %d     Accuracy of 0: %d\"%(round(acc),round(acc1),round(acc0)))\n","    print(\"AUC %.4f\"%(roc_auc_score(testout,predictions)))\n","    #print(precision_score(testin,predictions.round()))\n","    #print(recall_score(testin,predictions.round()))\n","    #print(f1_score(testin,predictions.round()))\n","########################################### collection of different function calls to balance the data  \n","#bal_x, bal_y =smote(3,X_train,Y_train)#get more balanced data using smote with the input and output as well as the K value \n","\n","#ad_x,ad_y=ada(X_train,Y_train)#get more balanced data using ADASYN\n","\n","#u_x,u_y=ENN(12,X_train,Y_train)\n","\n","#CNN_x,CNN_y=CNN(1,X_train,Y_train)#relativly slow do not use again \n","#print(\"Done ENN\")#takes a fair amount of time \n","#NM_x, NM_y=NM(3,X_train,Y_train)#try this one if they still take a long time \n","#print(\"Done NM\")#does not take long but does when using second type\n","#ONN_x,ONN_y=OSS(4,200,X_train, Y_train)\n","#print(\"Done ENN\")\n","#combination \n","\n","#cx,cy=comb(X_train,Y_train)#########################################################################################################################UNDO ON FIRST RUN\n","#mod=NNetwork(X_train,Y_train,10,100,600,250,1)#train using the X_train and Y_train with epoch and first and second layers and specify 1 if weig\n","\n","#####################################################different NN implimentations and printouts based on each data for comperison\n","\"\"\" print(\"SMOTE\")\n","mod=NNetwork(bal_x,bal_y,10,500,1000,280,400,0)#train using the X_train and Y_train with epoch and first and second layers and specify 1 if weig\n","print(\"With balanced training data\")\n","NNacc(mod, bal_x,bal_y)\n","print(\"With origional training data\")\n","NNacc(mod, X_train,Y_train)\n","print(\"with origional testing data\")\n","NNacc(mod, X_test, Y_test)\n","\n","\n","print(\"ADASYN\")\n","mod=NNetwork(ad_x,ad_y,10,500,1000,280,400,0)#train using the X_train and Y_train with epoch and first and second layers and specify 1 if weig\n","print(\"With balanced training data\")\n","NNacc(mod, ad_x,ad_y)\n","print(\"With origional training data\")\n","NNacc(mod, X_train,Y_train)\n","print(\"with origional testing data\")\n","NNacc(mod, X_test, Y_test) \n","  \n","\n","print(\"Edited NN\")\n","mod=NNetwork(u_x,u_y,10,100,100,280,400,0)#\n","print(\"With balanced training data\")\n","NNacc(mod, u_x,u_y)\n","print(\"With origional training data\")\n","NNacc(mod, X_train,Y_train)\n","print(\"with origional testing data\")\n","NNacc(mod, X_test, Y_test) \n","\n","\n","print(\"One Sided NN\")\n","mod=NNetwork(ONN_x,ONN_y,10,100,100,280,400,0)#\n","print(\"With balanced training data\")\n","NNacc(mod, ONN_x,ONN_y)\n","print(\"With origional training data\")\n","NNacc(mod, X_train,Y_train)\n","print(\"with origional testing data\")\n","NNacc(mod, X_test, Y_test) \n","\n","\n","print(\"Near Miss\")\n","mod=NNetwork(NM_x,NM_y,10,100,600,300,400,0)#\n","print(\"With balanced training data\")\n","NNacc(mod, NM_x,NM_y)\n","print(\"With origional training data\")\n","NNacc(mod, X_train,Y_train)\n","print(\"with origional testing data\")\n","NNacc(mod, X_test, Y_test) \n","\"\"\"\n","##########################################################################most results tracked in NN Testing doc on google drive \n","print(\"Combined\")\n","mod=NNetwork(cx,cy,10,500,500,320,400,0)#\n","print(\"With balanced training data\")\n","NNacc(mod, cx,cy)\n","print(\"With origional training data\")\n","NNacc(mod, X_train,Y_train)\n","print(\"with origional testing data\")\n","NNacc(mod, X_test, Y_test) \n","\n","\n","\"\"\" print(\"NN Added Weights\")\n","mod=NNetwork(X_train,Y_train,10,100,100,280,400,1)#train using the X_train and Y_train with epoch and first and second layers and specify 1 if weig\n","print(\"With balanced training data\")\n","NNacc(mod, X_train,Y_train)\n","print(\"with origional testing data\")\n","NNacc(mod, X_test, Y_test) \"\"\"\n"]}]}